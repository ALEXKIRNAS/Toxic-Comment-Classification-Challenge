{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - TF-IDF Classifier\n",
    "\n",
    "Ваша цель обучить классификатор который будет находить \"токсичные\" комментарии и опубликовать решения на Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "В процессе обучения нужно ответить на ***[вопросы](https://docs.google.com/forms/d/e/1FAIpQLSd9mQx8EFpSH6FhCy1M_FmISzy3lhgyyqV3TN0pmtop7slmTA/viewform?usp=sf_link)***\n",
    "\n",
    "Данные можно скачать тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./input/train.csv').fillna('Unknown')\n",
    "test = pd.read_csv('./input/test.csv').fillna('Unknown')\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "train_submission = pd.DataFrame.from_dict({'id': train['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "#re_tok = re.compile('([%s“”¨«»®´·º½¾¿¡§£₤‘’])' % string.punctuation)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azarichkovyi/Projects/Mask_RCNN/env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Layer\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, CuDNNLSTM, Add, Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv1D, GaussianNoise, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './input/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pseudo_labeling = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 300000\n",
    "maxlen = 200\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = train[class_names].values\n",
    "train = train[\"comment_text\"].fillna(\"fillna\").map(clean_text).values\n",
    "test = test[\"comment_text\"].fillna(\"fillna\").map(clean_text).values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train) + list(test))\n",
    "X_train = tokenizer.texts_to_sequences(train)\n",
    "X_test = tokenizer.texts_to_sequences(test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "pseudo_labeling_data, pseudo_labeling_targets = np.zeros(shape=(0, maxlen)), np.zeros(shape=(0, 6), dtype=np.int32)\n",
    "if use_pseudo_labeling:\n",
    "    pseudo_labeling_df = pd.read_csv('./submission_ensemble_005.csv')\n",
    "    pred = np.array(pseudo_labeling_df[class_names])\n",
    "    indexes_to_pick = np.all(((pred > 0.999) | (pred < 0.001)), axis=1)\n",
    "    \n",
    "    pseudo_labeling_data = x_test[indexes_to_pick, :]\n",
    "    pseudo_labeling_targets = np.round(pseudo_labeling_df[class_names].iloc[indexes_to_pick]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68964, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_labeling_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, batch_size=512, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            \n",
    "            \n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func    \n",
    "\n",
    "def get_gru_v2(dropout=0., dropout_dense=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = GaussianNoise(stddev=0.15)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Attention(maxlen)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def prepare_data_cv():\n",
    "    global targets_train, x_train, x_test\n",
    "    \n",
    "    kfold_data = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0xCAFFE)\n",
    "    \n",
    "    targets_train = np.array(targets_train)\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    for train_indices, val_indices in kf.split(targets_train):\n",
    "        X_train_cv = x_train[train_indices]\n",
    "        y_train_cv = targets_train[train_indices]\n",
    "\n",
    "        X_val = x_train[val_indices]\n",
    "        y_val = targets_train[val_indices]\n",
    "        \n",
    "        X_train_cv = np.vstack((X_train_cv, pseudo_labeling_data))\n",
    "        y_train_cv = np.vstack((y_train_cv, pseudo_labeling_targets))\n",
    "\n",
    "        kfold_data.append((X_train_cv, y_train_cv, X_val, y_val, val_indices))\n",
    "\n",
    "    X_test = x_test\n",
    "\n",
    "    return (kfold_data, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_callbacks(save_dir):\n",
    "    stopping = EarlyStopping(monitor='val_loss',\n",
    "                             min_delta=1e-3,\n",
    "                             patience=5,\n",
    "                             verbose=False,\n",
    "                             mode='min')\n",
    "\n",
    "    board_path = os.path.join(save_dir, 'board')\n",
    "    if not os.path.exists(board_path):\n",
    "        os.makedirs(board_path)\n",
    "\n",
    "    lr_sheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                    factor=0.1,\n",
    "                                    patience=2,\n",
    "                                    verbose=True,\n",
    "                                    mode='min',\n",
    "                                    epsilon=2e-3,\n",
    "                                    min_lr=1e-5)\n",
    "\n",
    "    model_path = os.path.join(save_dir, 'model/model_weights.hdf5')\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(model_path,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=False,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='min',\n",
    "                                       period=1)\n",
    "\n",
    "    callbacks = [stopping, lr_sheduler, model_checkpoint]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 200, 300)          90000000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise_7 (GaussianNo (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 200, 256)          330240    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 200, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 200, 256)          296448    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 200, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "attention_7 (Attention)      (None, 256)               456       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 90,663,374\n",
      "Trainable params: 662,094\n",
      "Non-trainable params: 90,001,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_gru_v2(dropout_dense=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 196620 samples, validate on 31915 samples\n",
      "Epoch 1/15\n",
      "196620/196620 [==============================] - 81s 412us/step - loss: 0.0897 - acc: 0.9738 - val_loss: 0.0521 - val_acc: 0.9817\n",
      "Epoch 2/15\n",
      "196620/196620 [==============================] - 79s 401us/step - loss: 0.0318 - acc: 0.9884 - val_loss: 0.0453 - val_acc: 0.9828\n",
      "Epoch 3/15\n",
      "196620/196620 [==============================] - 79s 400us/step - loss: 0.0292 - acc: 0.9889 - val_loss: 0.0433 - val_acc: 0.9833\n",
      "Epoch 4/15\n",
      "196620/196620 [==============================] - 79s 400us/step - loss: 0.0276 - acc: 0.9893 - val_loss: 0.0423 - val_acc: 0.9833\n",
      "Epoch 5/15\n",
      "196620/196620 [==============================] - 79s 401us/step - loss: 0.0264 - acc: 0.9895 - val_loss: 0.0416 - val_acc: 0.9842\n",
      "Epoch 6/15\n",
      "196620/196620 [==============================] - 78s 399us/step - loss: 0.0256 - acc: 0.9899 - val_loss: 0.0401 - val_acc: 0.9844\n",
      "Epoch 7/15\n",
      "196620/196620 [==============================] - 79s 400us/step - loss: 0.0249 - acc: 0.9901 - val_loss: 0.0430 - val_acc: 0.9828\n",
      "Epoch 8/15\n",
      "196620/196620 [==============================] - 79s 402us/step - loss: 0.0243 - acc: 0.9903 - val_loss: 0.0432 - val_acc: 0.9823\n",
      "Epoch 9/15\n",
      "196620/196620 [==============================] - 79s 400us/step - loss: 0.0240 - acc: 0.9904 - val_loss: 0.0403 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/15\n",
      "196620/196620 [==============================] - 78s 397us/step - loss: 0.0223 - acc: 0.9910 - val_loss: 0.0398 - val_acc: 0.9845\n",
      "Epoch 11/15\n",
      "196620/196620 [==============================] - 78s 397us/step - loss: 0.0221 - acc: 0.9912 - val_loss: 0.0401 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [15:31<1:02:07, 931.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC:\n",
      "Mean: 0.995879\n",
      "Std: 0.000000\n",
      "Min: 0.995879\n",
      "Max: 0.995879\n",
      "\n",
      "\n",
      "Val ROC AUC:\n",
      "Mean: 0.989693\n",
      "Std: 0.000000\n",
      "Min: 0.989693\n",
      "Max: 0.989693\n",
      "\n",
      "\n",
      "Train on 196621 samples, validate on 31914 samples\n",
      "Epoch 1/15\n",
      "196621/196621 [==============================] - 83s 420us/step - loss: 0.0823 - acc: 0.9770 - val_loss: 0.0537 - val_acc: 0.9798\n",
      "Epoch 2/15\n",
      "196621/196621 [==============================] - 78s 398us/step - loss: 0.0316 - acc: 0.9883 - val_loss: 0.0446 - val_acc: 0.9829\n",
      "Epoch 3/15\n",
      "196621/196621 [==============================] - 78s 396us/step - loss: 0.0293 - acc: 0.9888 - val_loss: 0.0445 - val_acc: 0.9823\n",
      "Epoch 4/15\n",
      "196621/196621 [==============================] - 78s 397us/step - loss: 0.0278 - acc: 0.9892 - val_loss: 0.0410 - val_acc: 0.9839\n",
      "Epoch 5/15\n",
      "196621/196621 [==============================] - 78s 398us/step - loss: 0.0268 - acc: 0.9895 - val_loss: 0.0406 - val_acc: 0.9843\n",
      "Epoch 6/15\n",
      "196621/196621 [==============================] - 78s 398us/step - loss: 0.0260 - acc: 0.9897 - val_loss: 0.0404 - val_acc: 0.9838\n",
      "Epoch 7/15\n",
      "196621/196621 [==============================] - 79s 401us/step - loss: 0.0253 - acc: 0.9898 - val_loss: 0.0395 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/15\n",
      "196621/196621 [==============================] - 79s 401us/step - loss: 0.0240 - acc: 0.9904 - val_loss: 0.0393 - val_acc: 0.9842\n",
      "Epoch 9/15\n",
      "196621/196621 [==============================] - 79s 402us/step - loss: 0.0237 - acc: 0.9905 - val_loss: 0.0387 - val_acc: 0.9847\n",
      "Epoch 10/15\n",
      "196621/196621 [==============================] - 79s 400us/step - loss: 0.0237 - acc: 0.9905 - val_loss: 0.0390 - val_acc: 0.9844\n",
      "Epoch 11/15\n",
      "196621/196621 [==============================] - 78s 398us/step - loss: 0.0235 - acc: 0.9905 - val_loss: 0.0391 - val_acc: 0.9844\n",
      "Epoch 12/15\n",
      "196621/196621 [==============================] - 78s 397us/step - loss: 0.0234 - acc: 0.9906 - val_loss: 0.0391 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [32:24<48:36, 972.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC:\n",
      "Mean: 0.995778\n",
      "Std: 0.000101\n",
      "Min: 0.995677\n",
      "Max: 0.995879\n",
      "\n",
      "\n",
      "Val ROC AUC:\n",
      "Mean: 0.989276\n",
      "Std: 0.000417\n",
      "Min: 0.988859\n",
      "Max: 0.989693\n",
      "\n",
      "\n",
      "Train on 196621 samples, validate on 31914 samples\n",
      "Epoch 1/15\n",
      "196621/196621 [==============================] - 82s 416us/step - loss: 0.0816 - acc: 0.9776 - val_loss: 0.0480 - val_acc: 0.9820\n",
      "Epoch 2/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0314 - acc: 0.9885 - val_loss: 0.0431 - val_acc: 0.9834\n",
      "Epoch 3/15\n",
      "196621/196621 [==============================] - 78s 399us/step - loss: 0.0287 - acc: 0.9891 - val_loss: 0.0443 - val_acc: 0.9825\n",
      "Epoch 4/15\n",
      "196621/196621 [==============================] - 79s 401us/step - loss: 0.0273 - acc: 0.9895 - val_loss: 0.0411 - val_acc: 0.9838\n",
      "Epoch 5/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0265 - acc: 0.9897 - val_loss: 0.0407 - val_acc: 0.9837\n",
      "Epoch 6/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0256 - acc: 0.9900 - val_loss: 0.0404 - val_acc: 0.9839\n",
      "Epoch 7/15\n",
      "196621/196621 [==============================] - 79s 401us/step - loss: 0.0249 - acc: 0.9902 - val_loss: 0.0399 - val_acc: 0.9841\n",
      "Epoch 8/15\n",
      "196621/196621 [==============================] - 79s 400us/step - loss: 0.0245 - acc: 0.9902 - val_loss: 0.0391 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/15\n",
      "196621/196621 [==============================] - 79s 401us/step - loss: 0.0229 - acc: 0.9909 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 10/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0227 - acc: 0.9909 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 11/15\n",
      "196621/196621 [==============================] - 79s 402us/step - loss: 0.0226 - acc: 0.9909 - val_loss: 0.0386 - val_acc: 0.9847\n",
      "Epoch 12/15\n",
      "196621/196621 [==============================] - 79s 402us/step - loss: 0.0224 - acc: 0.9910 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 13/15\n",
      "196621/196621 [==============================] - 79s 400us/step - loss: 0.0222 - acc: 0.9910 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/15\n",
      "196621/196621 [==============================] - 79s 402us/step - loss: 0.0220 - acc: 0.9911 - val_loss: 0.0387 - val_acc: 0.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [51:59<34:39, 1039.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC:\n",
      "Mean: 0.995962\n",
      "Std: 0.000272\n",
      "Min: 0.995677\n",
      "Max: 0.996328\n",
      "\n",
      "\n",
      "Val ROC AUC:\n",
      "Mean: 0.989133\n",
      "Std: 0.000396\n",
      "Min: 0.988847\n",
      "Max: 0.989693\n",
      "\n",
      "\n",
      "Train on 196621 samples, validate on 31914 samples\n",
      "Epoch 1/15\n",
      "196621/196621 [==============================] - 82s 417us/step - loss: 0.0783 - acc: 0.9780 - val_loss: 0.0468 - val_acc: 0.9825\n",
      "Epoch 2/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0309 - acc: 0.9886 - val_loss: 0.0440 - val_acc: 0.9828\n",
      "Epoch 3/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0287 - acc: 0.9891 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 4/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0272 - acc: 0.9895 - val_loss: 0.0415 - val_acc: 0.9838\n",
      "Epoch 5/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0263 - acc: 0.9896 - val_loss: 0.0396 - val_acc: 0.9846\n",
      "Epoch 6/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0254 - acc: 0.9900 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "Epoch 7/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0248 - acc: 0.9902 - val_loss: 0.0390 - val_acc: 0.9845\n",
      "Epoch 8/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0240 - acc: 0.9904 - val_loss: 0.0402 - val_acc: 0.9841\n",
      "Epoch 9/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0238 - acc: 0.9905 - val_loss: 0.0393 - val_acc: 0.9850\n",
      "Epoch 10/15\n",
      "196621/196621 [==============================] - 79s 403us/step - loss: 0.0231 - acc: 0.9908 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [1:06:20<16:35, 995.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC:\n",
      "Mean: 0.995874\n",
      "Std: 0.000280\n",
      "Min: 0.995613\n",
      "Max: 0.996328\n",
      "\n",
      "\n",
      "Val ROC AUC:\n",
      "Mean: 0.989270\n",
      "Std: 0.000417\n",
      "Min: 0.988847\n",
      "Max: 0.989693\n",
      "\n",
      "\n",
      "Train on 196621 samples, validate on 31914 samples\n",
      "Epoch 1/15\n",
      "196621/196621 [==============================] - 83s 422us/step - loss: 0.0783 - acc: 0.9783 - val_loss: 0.0451 - val_acc: 0.9831\n",
      "Epoch 2/15\n",
      "196621/196621 [==============================] - 80s 404us/step - loss: 0.0309 - acc: 0.9885 - val_loss: 0.0419 - val_acc: 0.9835\n",
      "Epoch 3/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0284 - acc: 0.9892 - val_loss: 0.0398 - val_acc: 0.9842\n",
      "Epoch 4/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0269 - acc: 0.9896 - val_loss: 0.0406 - val_acc: 0.9846\n",
      "Epoch 5/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0261 - acc: 0.9899 - val_loss: 0.0392 - val_acc: 0.9847\n",
      "Epoch 6/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0253 - acc: 0.9900 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0237 - acc: 0.9906 - val_loss: 0.0377 - val_acc: 0.9850\n",
      "Epoch 8/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0233 - acc: 0.9907 - val_loss: 0.0377 - val_acc: 0.9851\n",
      "Epoch 9/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0232 - acc: 0.9907 - val_loss: 0.0376 - val_acc: 0.9851\n",
      "Epoch 10/15\n",
      "196621/196621 [==============================] - 80s 405us/step - loss: 0.0230 - acc: 0.9907 - val_loss: 0.0375 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 11/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0229 - acc: 0.9908 - val_loss: 0.0374 - val_acc: 0.9850\n",
      "Epoch 12/15\n",
      "196621/196621 [==============================] - 79s 404us/step - loss: 0.0227 - acc: 0.9910 - val_loss: 0.0375 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [1:23:27<00:00, 1001.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC:\n",
      "Mean: 0.995870\n",
      "Std: 0.000250\n",
      "Min: 0.995613\n",
      "Max: 0.996328\n",
      "\n",
      "\n",
      "Val ROC AUC:\n",
      "Mean: 0.989606\n",
      "Std: 0.000768\n",
      "Min: 0.988847\n",
      "Max: 0.990948\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "STAMP = 'gru_109'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_gru_v2()\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_lstm_v2(dropout=0., dropout_dense=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = GaussianNoise(stddev=0.15)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Attention(maxlen)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_lstm_v2(dropout_dense=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "STAMP = 'lstm_101'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_lstm_v2()\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_text_cnn(dropout=0., dropout_dense=0., weight_decay=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    #x = GaussianNoise(stddev=0.1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=256, kernel_size=7, padding='same')(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Conv1D(filters=256, kernel_size=7, padding='same')(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Attention(maxlen // 2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_text_cnn(dropout_dense=0., weight_decay=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "STAMP = 'textcnn_100'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_text_cnn(dropout_dense=0.3, weight_decay=1e-4)\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
