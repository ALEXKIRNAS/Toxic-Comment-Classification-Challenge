{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - TF-IDF Classifier\n",
    "\n",
    "Ваша цель обучить классификатор который будет находить \"токсичные\" комментарии и опубликовать решения на Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "В процессе обучения нужно ответить на ***[вопросы](https://docs.google.com/forms/d/e/1FAIpQLSd9mQx8EFpSH6FhCy1M_FmISzy3lhgyyqV3TN0pmtop7slmTA/viewform?usp=sf_link)***\n",
    "\n",
    "Данные можно скачать тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./input/train.csv').fillna('Unknown')\n",
    "test = pd.read_csv('./input/test.csv').fillna('Unknown')\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "train_submission = pd.DataFrame.from_dict({'id': train['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#re_tok = re.compile('([%s“”¨«»®´·º½¾¿¡§£₤‘’])' % string.punctuation)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(' u ', 'you', text)\n",
    "    text = re.sub('\\nu ', 'you', text)\n",
    "    text = re.sub(' u\\n', 'you', text)\n",
    "    text = re.sub(\"fucksex\", 'fuck sex', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def cleanupDoc(s):\n",
    "    s = clean_text(s)\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stopset.add('wikipedia')\n",
    "    tokens = text_to_word_sequence(s, \n",
    "                                   filters=\"\\\"!'#$%&()*+,-˚˙./:;‘“<=·>?@[]^_`{|}~\\t\\n\",\n",
    "                                   lower=True,\n",
    "                                   split=\" \")\n",
    "    cleanup = \" \".join(filter(lambda word: word not in stopset, tokens))\n",
    "    return cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Layer\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, CuDNNLSTM, Add, Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv1D, GaussianNoise, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './input/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 300000\n",
    "maxlen = 200\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "train = train[\"comment_text\"].fillna(\"fillna\").map(clean_text).values\n",
    "test = test[\"comment_text\"].fillna(\"fillna\").map(clean_text).values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train) + list(test))\n",
    "X_train = tokenizer.texts_to_sequences(train)\n",
    "X_test = tokenizer.texts_to_sequences(test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, batch_size=512, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            \n",
    "            \n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func    \n",
    "\n",
    "def get_gru_v2(dropout=0., dropout_dense=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = GaussianNoise(stddev=0.15)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Attention(maxlen)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def prepare_data_cv():\n",
    "    global targets_train, x_train, x_test\n",
    "    \n",
    "    kfold_data = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0xCAFFE)\n",
    "    \n",
    "    targets_train = np.array(targets_train)\n",
    "    x_train = np.array(x_train)\n",
    "    \n",
    "    for train_indices, val_indices in kf.split(targets_train):\n",
    "        X_train_cv = x_train[train_indices]\n",
    "        y_train_cv = targets_train[train_indices]\n",
    "\n",
    "        X_val = x_train[val_indices]\n",
    "        y_val = targets_train[val_indices]\n",
    "\n",
    "        kfold_data.append((X_train_cv, y_train_cv, X_val, y_val, val_indices))\n",
    "\n",
    "    X_test = x_test\n",
    "\n",
    "    return (kfold_data, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_callbacks(save_dir):\n",
    "    stopping = EarlyStopping(monitor='val_loss',\n",
    "                             min_delta=1e-3,\n",
    "                             patience=5,\n",
    "                             verbose=False,\n",
    "                             mode='min')\n",
    "\n",
    "    board_path = os.path.join(save_dir, 'board')\n",
    "    if not os.path.exists(board_path):\n",
    "        os.makedirs(board_path)\n",
    "\n",
    "    lr_sheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                    factor=0.1,\n",
    "                                    patience=2,\n",
    "                                    verbose=True,\n",
    "                                    mode='min',\n",
    "                                    epsilon=2e-3,\n",
    "                                    min_lr=1e-5)\n",
    "\n",
    "    model_path = os.path.join(save_dir, 'model/model_weights.hdf5')\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(model_path,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=False,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='min',\n",
    "                                       period=1)\n",
    "\n",
    "    callbacks = [stopping, lr_sheduler, model_checkpoint]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_gru_v2(dropout_dense=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "STAMP = 'gru_107'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_gru_v2()\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_lstm_v2(dropout=0., dropout_dense=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = GaussianNoise(stddev=0.15)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    x = _bn_elu()(x)\n",
    "    \n",
    "    x = Attention(maxlen)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_lstm_v2(dropout_dense=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "STAMP = 'lstm_100'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_lstm_v2()\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bn_elu():\n",
    "    def func(x):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_text_cnn(dropout=0., dropout_dense=0., weight_decay=0.):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    #x = GaussianNoise(stddev=0.1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=256, kernel_size=7, padding='same')(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Conv1D(filters=256, kernel_size=7, padding='same')(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Attention(maxlen // 2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = _bn_elu()(x) \n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    outp = Dense(6, use_bias=True, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_text_cnn(dropout_dense=0., weight_decay=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "STAMP = 'textcnn_100'\n",
    "experiment_path = './experiments/%s' % STAMP\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "\n",
    "(kfold_data, X_test) = prepare_data_cv()\n",
    "\n",
    "\n",
    "train_probas = np.zeros(shape=(x_train.shape[0], 6))\n",
    "test_probas = np.zeros(shape=(x_test.shape[0], 6))\n",
    "\n",
    "models_roc = []\n",
    "models_train_roc = []\n",
    "\n",
    "\n",
    "for idx, data in enumerate(tqdm(kfold_data)):\n",
    "    X_train, y_train, X_valid, y_valid, val_indices = data\n",
    "\n",
    "    model = get_text_cnn(dropout_dense=0.3, weight_decay=1e-4)\n",
    "    callbacks = get_model_callbacks(save_dir=os.path.join(experiment_path, 'fold_%02d' % idx))\n",
    "\n",
    "    model.fit(X_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(X_valid, y_valid),\n",
    "               shuffle=True,\n",
    "               callbacks=callbacks, verbose=1)\n",
    "\n",
    "    model.load_weights(filepath=os.path.join(experiment_path, ('fold_%02d/model/model_weights.hdf5' % idx)))\n",
    "\n",
    "    proba = model.predict(X_train, batch_size=batch_size*2)\n",
    "    proba_val = model.predict(X_valid, batch_size=batch_size*2)\n",
    "    proba_test = model.predict(x_test, batch_size=batch_size*2)\n",
    "\n",
    "    models_roc.append(roc_auc_score(y_valid, proba_val))\n",
    "    models_train_roc.append(roc_auc_score(y_train, proba))\n",
    "    \n",
    "    train_probas[val_indices] += proba_val\n",
    "    test_probas += proba_test / 5.\n",
    "\n",
    "\n",
    "    print('Train ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_train_roc),\n",
    "                                                                 np.std(models_train_roc),\n",
    "                                                                 np.min(models_train_roc),\n",
    "                                                                 np.max(models_train_roc)))\n",
    "\n",
    "    print('Val ROC AUC:\\nMean: %f\\nStd: %f\\nMin: %f\\nMax: %f\\n\\n' % (np.mean(models_roc),\n",
    "                                                                 np.std(models_roc),\n",
    "                                                                 np.min(models_roc),\n",
    "                                                                 np.max(models_roc)))\n",
    "\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    train_submission[cls_name] = train_probas[:, i]\n",
    "train_submission.to_csv('train_%s.csv' % STAMP, index=False)\n",
    "\n",
    "for i, cls_name in enumerate(class_names):\n",
    "    submission[cls_name] = test_probas[:, i]\n",
    "submission.to_csv('submission_%s.csv' % STAMP, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
