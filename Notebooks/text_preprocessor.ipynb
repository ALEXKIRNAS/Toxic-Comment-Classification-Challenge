{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_bad_words_1 = pd.read_csv('badwords.txt', names=['wrong', 'right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_bad_words_2 =  pd.read_csv('correct_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_whitespace = correct_bad_words_2[correct_bad_words_2['wrong'].str.contains(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = pickle.load(open('oov', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_oov = correct_bad_words_2[correct_bad_words_2['wrong'].isin(oov)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 74, 44)"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_bad_words_1), len(with_whitespace), len(bad_words_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_bad_words = correct_bad_words_1.merge(with_whitespace, on=['wrong', 'right'], how='outer')\n",
    "correct_bad_words = correct_bad_words.merge(bad_words_oov, on=['wrong', 'right'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = set(correct_bad_words[correct_bad_words['right'].isnull()]['wrong'].str.lower().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = correct_bad_words[~correct_bad_words['right'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "corrections['id'] = corrections['right'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corections = corrections[corrections['wrong'].str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corections = corrections[~corrections['wrong'].str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corrects_encode = dict(zip(char_corections.wrong.str.lower(), char_corections.id))\n",
    "char_corrects_decode = dict(zip(char_corections.id, char_corections.right.str.lstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corrects = dict(zip(word_corections.wrong.str.lower(), word_corections.right.str.lstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corrects_encode = OrderedDict(sorted(char_corrects_encode.items(), key=lambda t: len(t[0]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl = {\n",
    "    'macedonians': 'macedonian people',\n",
    "    'unsalvageably': 'irrecoverably',\n",
    "    'ukrainians': 'ukrainian people',\n",
    "    'assyrians': 'assyrian people',\n",
    "    'copyvios': 'copyvio',\n",
    "    'yugoslav': 'yugoslavia',\n",
    "    'deletionists': 'deletions',\n",
    "    'meatpuppet': 'meat puppet',\n",
    "    'meatpuppetry': 'meat puppet',\n",
    "    'sockpuppet': 'sock puppet',\n",
    "    'sockpuppeting': 'sock puppet',\n",
    "    'herzegovina': 'bosnia',\n",
    "    'azerbaijani': 'azerbaijan',\n",
    "    'watchlisted': 'watchlist',\n",
    "    'srebrenica': 'bosnia',\n",
    "    'moldovan': 'moldova',\n",
    "    'azeri': 'caucasian',\n",
    "    'sockmaster': 'sock master',\n",
    "    'sockpuppetter': 'sock master',\n",
    "    'niggerpuppets': 'nigger puppets',\n",
    "    'belarusian': 'ukrainian',\n",
    "    'jewishness': 'cheapness',\n",
    "    'vandalisim': 'vandalism',\n",
    "    'vandolism': 'vandalism',\n",
    "    'uzbek': 'uzbekistan people',\n",
    "    'tajik': 'uzbekistan people',\n",
    "    'wikinazi': 'wiki nazi',\n",
    "    'deletin': 'deleting',\n",
    "    'bulgars': 'bulgarians',\n",
    "    'islams': 'islam people',\n",
    "    'voluntaryslave': 'voluntary slave',\n",
    "    'assyrians': 'syrians',\n",
    "    'deleate': 'delete'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_and_digits(text, regexps):\n",
    "    for regexp in regexps:\n",
    "        result = regexp.match(text)\n",
    "        if result is not None:\n",
    "            return ' '.join(result.groups())\n",
    "    return text\n",
    "\n",
    "def _is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "\n",
    "QUOTED_STRING_RE = re.compile(\n",
    "    r\"(?P<quote>\\\"\\\")(?P<string>.*?)(?<!\\\\)(?P=quote)\")\n",
    "\n",
    "\n",
    "def parse_quoted_string(search_string):\n",
    "    r\"\"\"\n",
    "    >>> s = '...And the gold of \\'the knight\\\\\\'s good banner\\' Still waved...'\n",
    "    >>> parse_quoted_string(s)\n",
    "    \"the knight\\\\'s good banner\"\n",
    "    >>> s = '\"To save my lady!\" Fast rode \\'the knight\\'... by \"Stephen Crane\"'\n",
    "    >>> parse_quoted_string(s)\n",
    "    'To save my lady!'\n",
    "    >>> print(QUOTED_STRING_RE.findall(s))\n",
    "    [('\"', 'To save my lady!'), (\"'\", 'the knight'), ('\"', 'Stephen Crane')]\n",
    "    \"\"\"\n",
    "    match = QUOTED_STRING_RE.search(search_string)\n",
    "    if match:\n",
    "        if match.group('string').strip() in bad_words:\n",
    "            search_string = re.sub(QUOTED_STRING_RE, 'bad word', search_string)\n",
    "    return search_string\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# tokenizer=TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(r\"\\n\",\". \",comment)\n",
    "    comment=re.sub(r\"\\\\n\\n\",\". \",comment)\n",
    "    \n",
    "    #Special cases\n",
    "    comment=re.sub(r\"fucksex\",\"fuck sex\",comment)\n",
    "    comment=re.sub(r\"anti-semitism\",\"antisemitism\",comment)\n",
    "    \n",
    "    #Chinese bad word\n",
    "    comment=re.sub(r\"幹\",\"fuck\",comment)\n",
    "    comment=re.sub(r\"死\",\"die\",comment)\n",
    "    comment=re.sub(r\"他妈的\",\"fuck\",comment)\n",
    "    comment=re.sub(r\"去你妈的\",\"fuck off\",comment)\n",
    "    comment=re.sub(r\"肏你妈\",\"fuck your mother\",comment)\n",
    "    comment=re.sub(r\"肏你祖宗十八代\",\"your ancestors to the 18th generation\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "#     comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "\n",
    "\n",
    "    comment = re.sub(r\"what's\", \"what is \", comment)\n",
    "    comment = re.sub(r\"\\'s\", \" \", comment)\n",
    "    comment = re.sub(r\"\\'ve\", \" have \", comment)\n",
    "    comment = re.sub(r\"can't\", \"cannot \", comment)\n",
    "    comment = re.sub(r\"n't\", \" not \", comment)\n",
    "    comment = re.sub(r\"i'm\", \"i am \", comment)\n",
    "    comment = re.sub(r\"\\'re\", \" are \", comment)\n",
    "    comment = re.sub(r\"\\'d\", \" would \", comment)\n",
    "    comment = re.sub(r\"\\'ll\", \" will \", comment)\n",
    "#     comment = re.sub(r\"wtf\",\"what the fuck\", comment)    \n",
    "    comment = re.sub(r\"I\", \"one\", comment)\n",
    "    comment = re.sub(r\"II\", \"two\", comment)\n",
    "    comment = re.sub(r\"III\", \"three\", comment)\n",
    "    comment = re.sub(r'牛', \"cow\", comment)\n",
    "    comment=re.sub(r\"mothjer\",\"mother\",comment)\n",
    "    comment=re.sub(r\"g e t  r i d  o f  a l l  i  d i d  p l e a s e  j a ck a s s\",\n",
    "                   \"get rid of all i did please jackass\",comment)\n",
    "    comment=re.sub(r\"withought\",\"with out\",comment)\n",
    "    \n",
    "    \n",
    "    #Replace LINKS\n",
    "    comment = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', comment)\n",
    "    \n",
    "#     comment=substitute_repeats(comment)\n",
    "    comment = re.sub(r'(.)\\1{2,}', r'\\1', comment)\n",
    "\n",
    "    \n",
    "    #Bad chars correction\n",
    "    comment = ' ' +  comment\n",
    "    for wrong, right in char_corrects_encode.items():\n",
    "        comment = comment.replace(' ' + wrong,  ' __'+ str(right) + '__ ')\n",
    "#     print(comment)\n",
    "    for wrong, right in char_corrects_decode.items():\n",
    "        comment = comment.replace( '__'+ str(wrong) + '__', ' ' + right + ' ')\n",
    "#     print(comment)\n",
    "#     comment = parse_quoted_string(comment)\n",
    "     \n",
    "    comment = comment.replace('&', ' and ')\n",
    "    comment = comment.replace('@', ' at ')\n",
    "    comment = comment.replace('雲水','')\n",
    "    \n",
    "    #Tokenize\n",
    "    words=tokenizer.tokenize(comment)   \n",
    "    regexps = [re.compile(\"([a-zA-Z]+)([0-9]+)\"), re.compile(\"([0-9]+)([a-zA-Z]+)\")]\n",
    "    words = [split_text_and_digits(token, regexps) for token in words]\n",
    "    words = ' '.join(words).split()\n",
    "    #Bad words correction\n",
    "    words=[word_corrects[word] if word in word_corrects else word for word in words]\n",
    "    words=[repl[word] if word in repl else word for word in words]\n",
    "    words = ' '.join(words).split()\n",
    "    \n",
    "    words = list(filter(_is_valid_word, words))\n",
    "    \n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "does any1 think its dangerous to b on facebook\t\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'damn preceeding unsigned yada yada what a'"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\"damn 'preceeding unsigned.....yada...yada' What a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "stfu u deleted our page. tool.\t\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stfu u deleted our page tool'"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stfu u deleted our page tool'"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nHey folks!  Let\\'s watch throwing around the \"\"r\"\" word so loosely .... that constitutes a personal attack, and violates WP:CIVIL.   \"'"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 8 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test['prep_text'] = test['comment_text'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 152 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['prep_text'] = train['comment_text'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('data/test_prep.csv', index=False)\n",
    "train.to_csv('data/train_prep.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
