{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"models/gru_att_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import pickle\n",
    "numpy.random.seed(42)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import model_from_json, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 300000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer \n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "TOKEN_LINK = \"URL\"\n",
    "PUNCTUATION = {'.', '?', '!', ';', ''}\n",
    "\n",
    "def replace_links(text, token=TOKEN_LINK):\n",
    "    # some links begin with http or https: https?://\\S+\n",
    "    # link can begins with www: www\\.\\S+\n",
    "    # links to google documents: \\\\bgoo\\.gl/\\w+/?\n",
    "    # else:\n",
    "    #   hostname usually have some domain names separated by dot: (\\w+\\.){2,4}\n",
    "    #   last domain name can have min 2 and max 6 letters: ([a-z]{2,6})\n",
    "    #   if path exists - it can be separated by / and must begins with /: (/\\S+)*\n",
    "    #   link can have / in the end: /?\n",
    "    # to match some links with 1 dot we check if exists path after hostname:\n",
    "    #   hostname: \\w+\\.([a-z]{2,6})\n",
    "    #   path: /(\\S+/)*(\\S+)?\n",
    "    token = ' ' + token + ' '\n",
    "    return re.sub(\"https?://\\S+|www\\.\\S+|\\\\bgoo\\.gl/\\w+/?|\"\n",
    "                  \"(\\w+\\.){2,4}([a-z]{2,6})(/\\S+)*/?|\\w+\\.([a-z]{2,6})/(\\S+/)*(\\S+)?\", token, text)\n",
    "\n",
    "\n",
    "def tokenize(text, link_token=TOKEN_LINK):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "#     text = re.sub(' u ', 'you', text)\n",
    "#     text = re.sub('\\nu ', 'you', text)\n",
    "#     text = re.sub(' u\\n', 'you', text)    \n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(\"fuck\", ' fuck ', text)\n",
    "    text = re.sub(\"idiot\", ' idiot ', text)\n",
    "    text = re.sub(\"stupid\", ' stupid ', text)\n",
    "    text = re.sub(\"dick\", ' dick ', text)\n",
    "    text = re.sub(\"shit\", ' shit ', text)\n",
    "    text = text.strip(' ')\n",
    "#     return text\n",
    "    \n",
    "    text = re.sub(r\"_\", \" \", text)\n",
    "    text = re.sub(r\"-\", \" \", text)\n",
    "\n",
    "    text = replace_links(text, link_token)\n",
    "\n",
    "    tokens = [t for t in tweet_tokenizer.tokenize(text)]\n",
    "    # Remove hashtags\n",
    "    tokens=[re.sub(r'#', '', i) for i in tokens]\n",
    "    tokens=['.' if i == '..' else i for i in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer\n",
    "pkl_file = open(MODEL_NAME+'_tokenizer', 'rb')\n",
    "tok = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv1D, GaussianNoise\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        a = K.exp(ait)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "#         weighted_input = x * a\n",
    "#         return K.sum(weighted_input, axis=1)\n",
    "        return a\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(MODEL_NAME + '.h5', {'AttentionWithContext': AttentionWithContext}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for word, index in tok.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = tokenize(text)\n",
    "    return sequence.pad_sequences(tok.texts_to_sequences([text]), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(text):\n",
    "    rez = []\n",
    "    word_indexes = preprocess(text)\n",
    "    layer_outs = functor([word_indexes, 0.])\n",
    "    attention_weights = [w[0] for w in layer_outs[-7][0]]\n",
    "    for word_index, attention in zip(word_indexes[0], attention_weights):\n",
    "#         if word_index != 0:\n",
    "            rez.append((index_to_word.get(word_index, 0), attention))\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(attention, percentile=95):\n",
    "    summary = []\n",
    "    threshold = np.percentile([a[1]for a in attention], percentile)\n",
    "    for word, weight in attention:\n",
    "        if weight > threshold:\n",
    "            summary.append(word)\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my niggeez auranor know up neea know how 2 agree wit a mutha fuck a niggaz\n",
      "Predicttion: [[0.98616725 0.21777329 0.97447383 0.02286493 0.91591907 0.925846  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fuck', 0.4432374),\n",
       " ('mutha', 0.2418755),\n",
       " ('niggaz', 0.17667122),\n",
       " ('a', 0.1127302),\n",
       " ('wit', 0.013007024),\n",
       " ('a', 0.011463671),\n",
       " ('agree', 0.00039864704),\n",
       " ('up', 0.00028695018),\n",
       " ('2', 0.00021746528),\n",
       " ('how', 5.9453516e-05),\n",
       " ('know', 2.7019723e-05),\n",
       " ('my', 1.7767823e-05),\n",
       " ('know', 7.707409e-06),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0),\n",
       " (0, 0.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "*My niggeez Auranor know 'sup. Neea know how 2 agree wit a muthafucka, niggaz. \n",
    "\"\"\"\n",
    "attention = get_attention(text)\n",
    "print(tokenize(text))\n",
    "print('Predicttion:', model.predict(preprocess(text)))\n",
    "# get_summary(attention)\n",
    "sorted(attention, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
